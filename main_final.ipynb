{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "from math import atan2, cos, sin, sqrt, pi, log\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from numpy import linalg as LA\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import *\n",
    "from ants_dataset import *\n",
    "from utils_final import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the directories and data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the original image and mask directories\n",
    "original_images_dir = '/content/train'\n",
    "original_masks_dir = '/content/train_masks'\n",
    "\n",
    "# Paths to the new directories\n",
    "train_images_aug_dir = '/content/train_augmented'\n",
    "train_masks_aug_dir = '/content/train_masks_augmented'\n",
    "test_images_dir = '/content/test'\n",
    "test_masks_dir = '/content/test_masks'\n",
    "\n",
    "# Create the new directories if they do not exist\n",
    "os.makedirs(train_images_aug_dir, exist_ok=True)\n",
    "os.makedirs(train_masks_aug_dir, exist_ok=True)\n",
    "os.makedirs(test_images_dir, exist_ok=True)\n",
    "os.makedirs(test_masks_dir, exist_ok=True)\n",
    "\n",
    "# Get a list of all image and mask file names\n",
    "image_files = sorted(os.listdir(original_images_dir)) \n",
    "mask_files = sorted(os.listdir(original_masks_dir)) \n",
    "\n",
    "assert len(image_files) == len(mask_files), \"Mismatch between images and masks count.\"\n",
    "\n",
    "train_images, test_images, train_masks, test_masks = train_test_split(\n",
    "    image_files, mask_files, test_size=0.4, random_state=42)\n",
    "\n",
    "# Move training/testing images and masks to their respective directories\n",
    "move_files(train_images, original_images_dir, train_images_aug_dir)\n",
    "move_files(train_masks, original_masks_dir, train_masks_aug_dir)\n",
    "\n",
    "move_files(test_images, original_images_dir, test_images_dir)\n",
    "move_files(test_masks, original_masks_dir, test_masks_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AntsDataset(images_path=train_images_aug_dir, masks_path=train_masks_aug_dir, augmentation=True)\n",
    "test_dataset = AntsDataset(images_path=test_images_dir, masks_path=test_masks_dir, augmentation=False)\n",
    "\n",
    "train_dataset.load_patches(training=True)\n",
    "test_dataset.load_patches(training=False)\n",
    "\n",
    "# Augments only the training dataset\n",
    "augmented_images = torch.stack([img for img in train_dataset.loaded_patches_rgb])\n",
    "augmented_masks = torch.stack([label.squeeze(0) for label in train_dataset.loaded_patches_gt]) \n",
    "\n",
    "#Create a Tensor with the augmented dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(augmented_images, augmented_masks)\n",
    "generator = torch.Generator().manual_seed(25)\n",
    "\n",
    "# split the test_dataset into validation and testing\n",
    "test_dataset, val_dataset = random_split(test_dataset, [0.5, 0.5], generator=generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = torch.cuda.device_count() * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              num_workers=num_workers, pin_memory=False,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            num_workers=num_workers, pin_memory=False,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                            num_workers=num_workers, pin_memory=False,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "\n",
    "model = UNet(in_channels=3, num_classes=4).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "weights = torch.tensor([0.54, 0.73, 1.0, 0.06])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "# Initialize the metrics\n",
    "train_accuracies = []\n",
    "train_f1s = []\n",
    "val_accuracies = []\n",
    "val_f1s = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    train_running_accuracy = 0\n",
    "    train_running_f1 = 0\n",
    "\n",
    "    for idx, img_mask in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        img = img_mask[0].float().to(device)\n",
    "        mask = img_mask[1].long().to(device)\n",
    "\n",
    "        # Remove channel dimension (if mask has shape [B, 1, H, W])\n",
    "        mask = mask.squeeze(1)\n",
    "\n",
    "        # get the predictions\n",
    "        y_pred = model(img)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute training metrics\n",
    "        loss = criterion(y_pred, mask)\n",
    "        accuracy = compute_accuracy(y_pred, mask)\n",
    "        f1 = compute_f1_score(y_pred, mask, num_classes=y_pred.shape[1])\n",
    "\n",
    "        train_running_accuracy += accuracy\n",
    "        train_running_f1 += f1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate average metrics over the training set\n",
    "    train_accuracy = train_running_accuracy / (idx + 1)\n",
    "    train_f1 = train_running_f1 / (idx + 1)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_f1s.append(train_f1)\n",
    "\n",
    "    # Compute accuracy and f1_score on the validation set\n",
    "    val_accuracy, val_f1 = eval_model(val_dataloader, model, device)\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Training Accuracy EPOCH {epoch + 1}: {train_accuracy:.4f}\")\n",
    "    print(f\"Training F1 Score EPOCH {epoch + 1}: {train_f1:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Validation Accuracy EPOCH {epoch + 1}: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score EPOCH {epoch + 1}: {val_f1:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Save the model and metrics\n",
    "    save_model(model, epoch)\n",
    "\n",
    "plot_training_metrics(EPOCHS, train_accuracies, val_accuracies, train_f1s, val_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best epoch and save it\n",
    "best_score = 0 \n",
    "for epoch in range(len(val_f1s)):\n",
    "    f1_value= val_f1s[epoch]\n",
    "    accuracy = val_accuracies[epoch]\n",
    "    \n",
    "    score = (f1_value + accuracy) / 2\n",
    "\n",
    "    if score > best_score:\n",
    "        best_epoch = epoch\n",
    "        best_score = score\n",
    "print(f'best epoch {best_epoch+1}, Validation Accuracy : {val_accuracies[best_epoch]}, Validation F1 Score : {val_f1s[best_epoch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for the best epoch\n",
    "model_path = f'/content/models/unet_epoch_{best_epoch}.pth'\n",
    "trained_model = UNet(in_channels=3, num_classes=4).to(device)\n",
    "trained_model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy and f1_score on the test set\n",
    "test_accuracy, test_f1 = eval_model(test_dataloader, trained_model, device)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 random normalized images, predictions and masks of the test set\n",
    "n = 3\n",
    "\n",
    "image_tensors = []\n",
    "mask_tensors = []\n",
    "image_paths = []\n",
    "\n",
    "for _ in range(n):\n",
    "    random_index = random.randint(0, len(test_dataloader.dataset) - 1)\n",
    "    random_sample = test_dataloader.dataset[random_index]\n",
    "\n",
    "    image_tensors.append(random_sample[0])\n",
    "    mask_tensors.append(random_sample[1])\n",
    "\n",
    "mask_pred_plots(image_tensors, mask_tensors, trained_model, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
